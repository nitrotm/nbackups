#!/usr/bin/python
"""
    This program is free software: you can redistribute it and/or modify
    it under the terms of the GNU General Public License as published by
    the Free Software Foundation, either version 3 of the License, or
    (at your option) any later version.

    This program is distributed in the hope that it will be useful,
    but WITHOUT ANY WARRANTY; without even the implied warranty of
    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
    GNU General Public License for more details.

    You should have received a copy of the GNU General Public License
    along with this program.  If not, see <http://www.gnu.org/licenses/>.


    Authors:
    -------

    Antony Ducommun <nitro@tmsrv.org>
"""

import argparse, datetime, hashlib, io, json, os, stat, struct, tarfile, tempfile
from Crypto.Cipher import AES
from Crypto import Random

config_file = os.path.expanduser('~/.nbackups')

def read_config():
	try:
		with io.open(config_file, 'r') as f:
			return json.load(f)
	except:
		print("Cannot read configuration:", sys.exc_info()[0])
		return dict()

def write_config():
	try:
		with io.open(config_file, 'w') as f:
			json.dump(config, f)
		os.chmod(config_file, stat.S_IRUSR | stat.S_IWUSR)
	except:
		print("Cannot write configuration:", sys.exc_info()[0])


config = read_config()

if 'options' in config:
	options = dict(config['options'])
else:
	options = dict()

if 'password' in options:
	password = str(options['password'])
else:
	password = None

if 'saltSize' in options:
	salt_size = int(options['saltSize'])
else:
	salt_size = 0

if 'pbkdf2Iterations' in options:
	pbkdf2_iterations = int(options['pbkdf2Iterations'])
else:
	pbkdf2_iterations = 0

if 'keySize' in options:
	key_size = int(options['keySize'])
else:
	key_size = 16

if 'targetPath' in options:
	target_path = str(options['targetPath'])
else:
	target_path = None

if 'maxBackups' in options:
	max_backups = max(1, int(options['maxBackups']))
else:
	max_backups = None

if 'paths' in config:
	paths = set(config['paths'])
else:
	paths = set()


random = Random.new()


def sha256(*values):
	digest = hashlib.sha256()
	for value in values:
		digest.update(value)
	return digest.digest()

def sha256_file(f, offset=0, size=None):
	digest = hashlib.sha256()
	if not size:
		size = f.seek(os.SEEK_END)
	f.seek(offset)
	end = offset + size
	while offset < end:
		bufferSize = end - offset
		if bufferSize > 1024:
			bufferSize = 1024
		buffer = f.read(bufferSize)
		offset += len(buffer)
		digest.update(buffer)
	return digest.digest()


_salt = None

def salt():
	global _salt

	if not _salt:
		_salt = random.read(salt_size)
	return _salt


def pbkdf2(size, salt=salt(), iterations=pbkdf2_iterations):
	password_raw = bytes(password, 'utf-8')
	index = 0
	digest = bytes()
	while len(digest) < size:
		block = bytearray(sha256(password_raw, salt, struct.pack('L', index)))
		for i in range(0, iterations):
			cur = sha256(password_raw, block)
			for j in range(0, len(block)):
				block[j] = block[j] ^ cur[j]
		digest += block
		index += 1
	return digest[0:size]


_key = None

def key():
	global _key

	if not _key:
		_key = pbkdf2(key_size)
	return _key


def pad(data, blockSize):
	if (len(data) % blockSize) != 0:
		return data + random.read(blockSize - len(data) % blockSize)
	return data


def id(path):
	path_raw = bytes(path, 'utf-8')
	digest = hashlib.sha256()
	digest.update(path_raw)
	return digest.hexdigest()


def find_by_id(id, parent_path=target_path):
	backups = list()
	for item in os.listdir(parent_path):
		item_path = os.path.join(parent_path, item)
		if os.path.isdir(item_path):
			backups = backups + find_by_id(id, item_path)
		if os.path.isfile(item_path) and item.find(id) == 0 and item.rfind('.bak') == (len(item) - len('.bak')):
			item_ts = item[len(id) + 1:len(id) + 15]
			time = datetime.datetime.strptime(item_ts, '%Y%m%d%H%M%S').replace(tzinfo=datetime.timezone.utc)
			stat = os.stat(item_path)
			backups.append({'path': item_path, 'time': time, 'size': stat.st_size})
	def backup_time(item):
		return item['time']
	backups.sort(key=backup_time, reverse=True)
	return backups


def find_most_recent_change(parent_path):
	most_recent = None
	for item in os.listdir(parent_path):
		item_path = os.path.join(parent_path, item)
		if os.path.isdir(item_path):
			time = find_most_recent_change(item_path)
			if time and (not most_recent or time > most_recent):
				most_recent = time
		if os.path.isfile(item_path):
			time = datetime.datetime.fromtimestamp(os.path.getmtime(item_path), tz=datetime.timezone.utc)
			if not most_recent or time > most_recent:
				most_recent = time
	return most_recent


def backup(args):
	path = os.path.realpath(args.path)
	path_raw = bytes(path, 'utf-8')
	path_id = id(path)

	print('backing up path: %s...' % path)

	# check if path is registered
	if not path in paths:
		print('  FAILED (path not registred)')
		return

	# check if path exists
	if not os.path.isdir(path):
		print('  FAILED (not found)')
		return

	# find last backup and check if any file changed since last backup
	backups = find_by_id(path_id)
	changed = None
	if len(backups) > 0:
		last_backup = backups[0]
		most_recent = find_most_recent_change(path)
		if last_backup['time'] > most_recent:
			changed = False
		else:
			changed = True

	# make new backup if necessary
	if changed == None or changed:
		ts = datetime.datetime.now(tz=datetime.timezone.utc)
		with tempfile.TemporaryFile() as f:
			# compute empty checksum
			digest = sha256()

			# write private header
			f.write(struct.pack('III', len(path_raw), len(digest), 0) + path_raw + digest)

			# archive data
			data_offset = f.tell()
			with tarfile.open(mode='w:xz', fileobj=f) as tar:
				tar.add(path, arcname='')
			data_end = os.fstat(f.fileno()).st_size

			# compute data checksum and write private header again
			digest = sha256_file(f, data_offset, data_end - data_offset)
			f.seek(0)
			f.write(struct.pack('III', len(path_raw), len(digest), data_end - data_offset) + path_raw + digest)

			# compute unique iv and init cipher
			iv = random.read(AES.block_size)
			cipher = AES.new(key(), AES.MODE_CBC, iv)

			# encrypt data to output
			f.seek(0)
			with io.open('%s/%s_%s.bak' % (target_path, path_id, ts.strftime('%Y%m%d%H%M%S')), 'wb') as f2:
				# write public header
				f2.write(struct.pack('III', salt_size, pbkdf2_iterations, key_size) + salt() + iv)

				# encrypt
				bufferSize = AES.block_size * 1024
				while f.tell() < data_end:
					buffer = bytes()
					while len(buffer) < bufferSize and f.tell() < data_end:
						buffer = buffer + f.read(bufferSize - len(buffer))
					buffer = pad(buffer, AES.block_size)
					f2.write(cipher.encrypt(buffer))
				os.fsync(f2)
		if changed:
			print('  DONE(changed): %s' % ts.astimezone().strftime('%Y-%m-%d %H:%M:%S'))
		else:
			print('  DONE(new): %s' % ts.astimezone().strftime('%Y-%m-%d %H:%M:%S'))
	else:
		print('  UP-TO-DATE: %s' % backups[0]['time'].astimezone().strftime('%Y-%m-%d %H:%M:%S'))

	# remove obsolete backups
	backups = find_by_id(path_id)
	if max_backups and len(backups) > max_backups:
		for backup in backups[max_backups:]:
			os.unlink(backup['path'])
			print('  CLEANED: %s' % backup['time'].astimezone().strftime('%Y-%m-%d %H:%M:%S'))


def restore(args):
	path = os.path.realpath(args.path)
	target = os.path.realpath(args.target)
	path_id = id(path)

	print('restoring path: %s...' % path)

	# check if path is registered
	if not path in paths:
		print('  FAILED (not registred)')
		return

	# check that target is valid (must not exist already)
	if os.path.exists(target):
		print('  FAILED (target already exist)')
		return

	# list matching backups
	backups = find_by_id(path_id)
	if len(backups) == 0:
		print('  FAILED (not found)')
		return

	# select last backup
	last_backup = backups[0]
	with io.open(last_backup['path'], 'rb') as f:
		# read public header
		(local_salt_size, local_pbkdf2_iterations, local_key_size) = struct.unpack('III', f.read(3 * 4));
		local_salt = f.read(local_salt_size)
		iv = f.read(AES.block_size)
		data_end = os.stat(last_backup['path']).st_size

		# init cipher
		local_key = pbkdf2(local_key_size, local_salt, local_pbkdf2_iterations)
		cipher = AES.new(local_key, AES.MODE_CBC, iv)

		# decrypt data to temporary file
		with tempfile.TemporaryFile() as f2:
			# decrypt
			bufferSize = AES.block_size * 1024
			while f.tell() < data_end:
				buffer = bytes()
				while len(buffer) < bufferSize and f.tell() < data_end:
					buffer = buffer + f.read(bufferSize - len(buffer))
				f2.write(cipher.decrypt(buffer))

			# read private header
			f2.seek(0)
			(path_length, digest_length, tar_length) = struct.unpack('III', f2.read(3*4));
			if f2.tell() != 3 * 4:
				print('  FAILED (invalid data)')
				return
			local_path = f2.read(path_length)
			if len(local_path) != path_length:
				print('  FAILED (invalid data)')
				return
			digest = f2.read(digest_length)
			if len(digest) != digest_length:
				print('  FAILED (invalid data)')
				return
			tar_offset = f2.tell()
			if f2.seek(tar_length, os.SEEK_CUR) != tar_offset + tar_length:
				print('  FAILED (invalid data)')
				return

			# check path and digest
			if sha256_file(f2, tar_offset, tar_length) != digest:
				print('  FAILED (invalid digest)')
				return
			local_path = str(local_path, 'utf-8')
			if path != local_path:
				print('  FAILED (path does\'t match): %s' % local_path)

			# extract archive
			f2.seek(tar_offset)
			with tarfile.open(mode='r:xz', fileobj=f2) as tar:
				os.makedirs(target, 0o755)
				tar.extractall(target)
	print('  DONE: %s' % last_backup['time'].astimezone().strftime('%Y-%m-%d %H:%M:%S'))


def verify(args):
	path = os.path.realpath(args.path)
	path_id = id(path)

	print('verifying path: %s...' % path)

	# check if path is registered
	if not path in paths:
		print(' FAILED (not registred)')
		return False

	# list matching backups
	backups = find_by_id(path_id)
	if len(backups) == 0:
		print('  DONE (no backups)')
		return True

	# verify backups
	for backup in backups:
		tstext = backup['time'].strftime('%Y-%m-%d %H:%M:%S')
		with io.open(backup['path'], 'rb') as f:
			# read public header
			(local_salt_size, local_pbkdf2_iterations, local_key_size) = struct.unpack('III', f.read(3 * 4));
			local_salt = f.read(local_salt_size)
			iv = f.read(AES.block_size)
			data_end = os.stat(backup['path']).st_size

			# init cipher
			local_key = pbkdf2(local_key_size, local_salt, local_pbkdf2_iterations)
			cipher = AES.new(local_key, AES.MODE_CBC, iv)

			# decrypt data to temporary file
			with tempfile.TemporaryFile() as f2:
				# decrypt
				bufferSize = AES.block_size * 1024
				while f.tell() < data_end:
					buffer = bytes()
					while len(buffer) < bufferSize and f.tell() < data_end:
						buffer = buffer + f.read(bufferSize - len(buffer))
					f2.write(cipher.decrypt(buffer))

				# read private header
				f2.seek(0)
				(path_length, digest_length, tar_length) = struct.unpack('III', f2.read(3*4));
				if f2.tell() != 3 * 4:
					print('  FAILED (invalid data): %s' % tstext)
					return False
				local_path = f2.read(path_length)
				if len(local_path) != path_length:
					print('  FAILED (invalid data): %s' % tstext)
					return False
				digest = f2.read(digest_length)
				if len(digest) != digest_length:
					print('  FAILED (invalid data): %s' % tstext)
					return False
				tar_offset = f2.tell()
				if f2.seek(tar_length, os.SEEK_CUR) != tar_offset + tar_length:
					print('  FAILED (invalid data): %s' % tstext)
					return False

				# check path and digest
				if sha256_file(f2, tar_offset, tar_length) != digest:
					print('  FAILED (invalid digest): %s' % tstext)
					return False
				local_path = str(local_path, 'utf-8')
				if path != local_path:
					print('  FAILED (path does\'t match): %s' % local_path)
					return False
				print('  DONE: %s' % tstext)
	return True


def show(args):
	print('Configuration:')
	print('=============')
	for key in options:
		if key == 'password':
			print('%s: ***' % key)
		else:
			print('%s: %s' % (key, options[key]))
	print()
	print('Targets:')
	print('=======')
	_paths = list(paths)
	_paths.sort()
	for path in _paths:
		path_id = id(path)
		backups = find_by_id(path_id)
		print('%s: %s' % (path_id, path))
		for backup in backups:
			print('  %s: %.01f [mb]' % (backup['time'].astimezone().strftime('%Y-%m-%d %H:%M:%S'), backup['size'] / 1024.0 / 1024.0) )
		print()


def configure(args):
	if args.key == 'targetPath':
		options[args.key] = os.path.realpath(args.value)
	else:
		options[args.key] = args.value
	config['options'] = options
	write_config()


def add(args):
	_path = os.path.realpath(args.path)
	if len(_path) > 0 and _path not in paths:
		# check hash collisions
		path_id = id(_path)
		for item in paths:
			item_id = id(item)
			if item_id == path_id:
				print('cannot register path (collision): %s' % _path)
				return
		paths.add(_path)
		config['paths'] = list(paths)
		write_config()
		print('registered new path: %s' % _path)

		# do first backup if necessary
		backups = find_by_id(path_id)
		if len(backups) == 0:
			print()
			class BackupArgs(object):
				path = _path
			backup(BackupArgs())


def remove(args):
	path = os.path.realpath(args.path)
	if len(path) > 0 and path in paths:
		path_id = id(path)
		backups = find_by_id(path_id)
		paths.remove(path)
		print('unregistered path: %s' % path)
		config['paths'] = list(paths)
		write_config()

		# clean old backups
		if args.clean:
			for backup in backups:
				os.unlink(backup['path'])
				print('  CLEANED: %s' % backup['time'].astimezone().strftime('%Y-%m-%d %H:%M:%S'))


def sync(args):
	_paths = list(paths)
	_paths.sort()
	for item in _paths:
		class BackupArgs(object):
			path = item
		args2 = BackupArgs()
		backup(args2)
		if args.verify:
			verify(args2)
		print()


mainparser = argparse.ArgumentParser(description='Automated backups with encryption.')

subparsers = mainparser.add_subparsers()

parser = subparsers.add_parser('list')
parser.set_defaults(action=show)

parser = subparsers.add_parser('config')
parser.add_argument('key')
parser.add_argument('value')
parser.set_defaults(action=configure)

parser = subparsers.add_parser('add')
parser.add_argument('path')
parser.set_defaults(action=add)

parser = subparsers.add_parser('remove')
parser.add_argument('--clean', required=False, action='store_const', const=True)
parser.add_argument('path')
parser.set_defaults(action=remove)


parser = subparsers.add_parser('sync')
parser.add_argument('--verify', required=False, action='store_const', const=True)
parser.set_defaults(action=sync)

parser = subparsers.add_parser('backup')
parser.add_argument('path')
parser.set_defaults(action=backup)

parser = subparsers.add_parser('verify')
parser.add_argument('path')
parser.set_defaults(action=verify)

parser = subparsers.add_parser('restore')
parser.add_argument('path')
parser.add_argument('target')
parser.set_defaults(action=restore)


args = mainparser.parse_args()
if hasattr(args, 'action'):
	args.action(args)
